{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:05:48.423072100Z",
     "start_time": "2024-06-27T18:05:41.461198Z"
    }
   },
   "source": [
    "import sys\n",
    "import csv\n",
    "sys.path.append('..')\n",
    "from BDDData import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import circulant\n",
    "from product_graph import *\n",
    "from utils import *"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-27T18:05:49.483194200Z",
     "start_time": "2024-06-27T18:05:49.432072100Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:06:00.497603500Z",
     "start_time": "2024-06-27T18:05:50.016596600Z"
    }
   },
   "source": [
    "#Load dataframes\n",
    "bdd_data = BDD_dataset(\"raw_data/BDDdata/\")\n",
    "#Add column with the timestep\n",
    "bdd_data.add_timestep_id()\n",
    "#Add flags for chaotic values\n",
    "bdd_data.tag_chaotic(replace=True)\n",
    "#Compute the mod for the nazelle and wind angles\n",
    "bdd_data.angle_mod()\n",
    "#Interpolate the missing values\n",
    "bdd_data.interpolate_power()\n",
    "#Values smaller than 0 are set to 0\n",
    "bdd_data.cap_power_to_zero()\n",
    "#Normalize Patv feature to [0,1]\n",
    "bdd_data.normalize_power(min=0, max=1, method= \"MinMaxScaler\")\n",
    "#Convert df to matrix form, where only Patv is included. Then split into train, validation and test\n",
    "#The matrix contains the subset of the time series for ALL nodes, so an (TxN matrix)\n",
    "train, val, test = bdd_data.split_df()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:06:00.507758900Z",
     "start_time": "2024-06-27T18:06:00.500595400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set seed\n",
    "np.random.seed(42)\n",
    "# consider a p percentage of the data\n",
    "p = 0.1\n",
    "train_mask = np.random.choice(train.shape[1], int(train.shape[1] * p), replace=False)\n",
    "val_mask = np.random.choice(val.shape[1], int(val.shape[1] * p), replace=False)\n",
    "test_mask = np.random.choice(test.shape[1], int(test.shape[1] * p), replace=False)\n",
    "\n",
    "train = train[:, train_mask]\n",
    "val = val[:, val_mask]\n",
    "test = test[:, test_mask]"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:06:02.453840200Z",
     "start_time": "2024-06-27T18:06:02.448173200Z"
    }
   },
   "source": [
    "class CustomBDD_Dataset(data.Dataset):\n",
    "    def __init__(self, dataset, observation_window=12, forecast_window=12, starting_turbine = 0,  ending_turbine=133):\n",
    "        self.observation_window = observation_window\n",
    "        self.forecast_window = forecast_window\n",
    "        length = eval(f'len({dataset}[0])')#Retrieves length of dataset\n",
    "        bdd_data.get_observation_forecasting_window(time_series_len=length, observation_steps=self.observation_window, forecast_steps=self.forecast_window)#Generates obs window\n",
    "        self.window_of_interest =  bdd_data.sliding_indices[str(self.observation_window)+\",\"+str(self.forecast_window)]#Retrieves windows\n",
    "        self.starting_turbine = starting_turbine\n",
    "        self.ending_turbine = ending_turbine  \n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.window_of_interest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window = self.window_of_interest[idx]\n",
    "        if self.dataset == \"train\":\n",
    "            features = train[self.starting_turbine:self.ending_turbine+1,window[0]:window[1]]\n",
    "            labels = train[self.starting_turbine:self.ending_turbine+1,window[1]:window[2]]\n",
    "        elif self.dataset == \"val\":\n",
    "            features = val[self.starting_turbine:self.ending_turbine+1,window[0]:window[1]]\n",
    "            labels = val[self.starting_turbine:self.ending_turbine+1,window[1]:window[2]]\n",
    "        elif self.dataset == \"test\":\n",
    "            features = test[self.starting_turbine:self.ending_turbine+1,window[0]:window[1]]\n",
    "            labels = test[self.starting_turbine:self.ending_turbine+1,window[1]:window[2]]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return torch.from_numpy(features).float(), torch.from_numpy(labels).float()\n",
    "    \n",
    "obs_window = 12\n",
    "forecast_window = 12\n",
    "batch_size = 100\n",
    "num_nodes = 134\n",
    "\n",
    "train_dataset = CustomBDD_Dataset(\"train\",observation_window=obs_window,forecast_window=forecast_window)\n",
    "train_loader = data.DataLoader(train_dataset, shuffle=True, batch_size = batch_size)\n",
    "val_dataset = CustomBDD_Dataset(\"val\",observation_window=obs_window,forecast_window=forecast_window)\n",
    "val_loader = data.DataLoader(val_dataset, shuffle=True, batch_size = batch_size)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:06:03.278765500Z",
     "start_time": "2024-06-27T18:06:03.209265Z"
    }
   },
   "source": [
    "x,y = next(iter(train_loader))\n",
    "print(f\"{x.shape=}\\n{y.shape=}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=torch.Size([100, 134, 12])\n",
      "y.shape=torch.Size([100, 134, 12])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:06:03.756311900Z",
     "start_time": "2024-06-27T18:06:03.676599600Z"
    }
   },
   "source": [
    "G = nx.read_gml('data/spatial_graph_2000.gml')\n",
    "adj_mat = nx.adjacency_matrix(G)\n",
    "adj_mat = nx.to_numpy_array(G)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:06:04.497130200Z",
     "start_time": "2024-06-27T18:06:04.321178500Z"
    }
   },
   "source": [
    "S = normalize_adjacency(torch.tensor(adj_mat)).float().to(device)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:06:05.222995600Z",
     "start_time": "2024-06-27T18:06:05.214511Z"
    }
   },
   "source": [
    "print(adj_mat.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134, 134)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:06:05.952691700Z",
     "start_time": "2024-06-27T18:06:05.924233900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCNNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, matrix_powers, order):\n",
    "        super(GCNNLayer, self).__init__()\n",
    "        self.matrix_powers = matrix_powers\n",
    "        self.order = order\n",
    "        self.weights = nn.Parameter(torch.FloatTensor(in_features, out_features, order))\n",
    "        # use Xavier initialization to match variance of input with output\n",
    "        nn.init.xavier_uniform_(self.weights)\n",
    "\n",
    "    def forward(self, features):\n",
    "        batch_size = features.size(0)\n",
    "        output_dim = self.weights.size(1)\n",
    "        device = features.device\n",
    "\n",
    "        out = torch.zeros((batch_size, features.size(1), output_dim), device=device)\n",
    "        for k in range(self.order):\n",
    "            weighted = torch.bmm(features, self.weights[:, :, k].unsqueeze(0).repeat(batch_size, 1, 1))\n",
    "            shifted = torch.bmm(self.matrix_powers[k].unsqueeze(0).repeat(batch_size, 1, 1).to(device), weighted)\n",
    "            out += shifted\n",
    "        return out\n",
    "\n",
    "# Inputs must be sized [num_nodes, obs_size] and outputs will be [num_nodes, pred_size]\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self, hid_sizes, shift, order):\n",
    "        super(GCNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # compute matrix shift\n",
    "        matrix_powers = [torch.matrix_power(shift, k).float() for k in range(order)]\n",
    "        # input layer of size obs_size\n",
    "        self.layers.append(GCNNLayer(obs_window, hid_sizes[0], matrix_powers, order))\n",
    "        # num_hid hidden layers of size hid_size\n",
    "        for i in range(len(hid_sizes) - 1):\n",
    "            self.layers.append(GCNNLayer(hid_sizes[i], hid_sizes[i + 1], matrix_powers, order))\n",
    "\n",
    "    def forward(self, features):\n",
    "        temp = features\n",
    "        for layer in self.layers[:-1]:\n",
    "            # use relu activation function\n",
    "            temp = F.relu(layer(temp))\n",
    "        return self.layers[-1](temp)\n",
    "    \n",
    "model = GCNN([128,128, forecast_window], S,1).to(device)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Created GCNN model with {pytorch_total_params} parameters:\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.size())"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:06:24.506665500Z",
     "start_time": "2024-06-27T18:06:06.617678500Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device='cpu'):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = torch.nn.functional.mse_loss(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate_epoch(model, loader, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = torch.nn.functional.mse_loss(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device=device)\n",
    "        val_loss = evaluate_epoch(model, val_loader, device=device)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"epoch: {epoch}\\ttraining loss: {train_loss:.4f}\\tvalidation loss: {val_loss:.4f}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    print(f'Model training took {elapsed_time:.3f} seconds')\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\ttraining loss: 4.0753\tvalidation loss: 0.0634\n",
      "epoch: 2\ttraining loss: 0.0734\tvalidation loss: 0.0600\n",
      "epoch: 3\ttraining loss: 0.0736\tvalidation loss: 0.0618\n",
      "epoch: 4\ttraining loss: 0.0735\tvalidation loss: 0.0607\n",
      "epoch: 5\ttraining loss: 0.0735\tvalidation loss: 0.0610\n",
      "epoch: 6\ttraining loss: 0.0733\tvalidation loss: 0.0614\n",
      "epoch: 7\ttraining loss: 0.0738\tvalidation loss: 0.0606\n",
      "epoch: 8\ttraining loss: 0.0732\tvalidation loss: 0.0603\n",
      "epoch: 9\ttraining loss: 0.0732\tvalidation loss: 0.0597\n",
      "epoch: 10\ttraining loss: 0.0732\tvalidation loss: 0.0602\n",
      "epoch: 11\ttraining loss: 0.0731\tvalidation loss: 0.0612\n",
      "epoch: 12\ttraining loss: 0.0734\tvalidation loss: 0.0620\n",
      "epoch: 13\ttraining loss: 0.0735\tvalidation loss: 0.0602\n",
      "epoch: 14\ttraining loss: 0.0739\tvalidation loss: 0.0619\n",
      "epoch: 15\ttraining loss: 0.0737\tvalidation loss: 0.0624\n",
      "epoch: 16\ttraining loss: 0.0734\tvalidation loss: 0.0612\n",
      "epoch: 17\ttraining loss: 0.0732\tvalidation loss: 0.0607\n",
      "epoch: 18\ttraining loss: 0.0728\tvalidation loss: 0.0616\n",
      "epoch: 19\ttraining loss: 0.0734\tvalidation loss: 0.0610\n",
      "epoch: 20\ttraining loss: 0.0733\tvalidation loss: 0.0617\n",
      "epoch: 21\ttraining loss: 0.0738\tvalidation loss: 0.0629\n",
      "epoch: 22\ttraining loss: 0.0736\tvalidation loss: 0.0621\n",
      "epoch: 23\ttraining loss: 0.0730\tvalidation loss: 0.0612\n",
      "epoch: 24\ttraining loss: 0.0734\tvalidation loss: 0.0601\n",
      "epoch: 25\ttraining loss: 0.0732\tvalidation loss: 0.0607\n",
      "epoch: 26\ttraining loss: 0.0733\tvalidation loss: 0.0619\n",
      "epoch: 27\ttraining loss: 0.0735\tvalidation loss: 0.0602\n",
      "epoch: 28\ttraining loss: 0.0732\tvalidation loss: 0.0604\n",
      "epoch: 29\ttraining loss: 0.0732\tvalidation loss: 0.0617\n",
      "epoch: 30\ttraining loss: 0.0734\tvalidation loss: 0.0611\n",
      "epoch: 31\ttraining loss: 0.0734\tvalidation loss: 0.0617\n",
      "epoch: 32\ttraining loss: 0.0735\tvalidation loss: 0.0611\n",
      "epoch: 33\ttraining loss: 0.0737\tvalidation loss: 0.0601\n",
      "epoch: 34\ttraining loss: 0.0736\tvalidation loss: 0.0620\n",
      "epoch: 35\ttraining loss: 0.0741\tvalidation loss: 0.0601\n",
      "epoch: 36\ttraining loss: 0.0747\tvalidation loss: 0.0622\n",
      "epoch: 37\ttraining loss: 0.0735\tvalidation loss: 0.0609\n",
      "epoch: 38\ttraining loss: 0.0735\tvalidation loss: 0.0615\n",
      "epoch: 39\ttraining loss: 0.0731\tvalidation loss: 0.0600\n",
      "epoch: 40\ttraining loss: 0.0729\tvalidation loss: 0.0609\n",
      "epoch: 41\ttraining loss: 0.0730\tvalidation loss: 0.0614\n",
      "epoch: 42\ttraining loss: 0.0732\tvalidation loss: 0.0591\n",
      "epoch: 43\ttraining loss: 0.0737\tvalidation loss: 0.0606\n",
      "epoch: 44\ttraining loss: 0.0741\tvalidation loss: 0.0614\n",
      "epoch: 45\ttraining loss: 0.0737\tvalidation loss: 0.0608\n",
      "epoch: 46\ttraining loss: 0.0734\tvalidation loss: 0.0608\n",
      "epoch: 47\ttraining loss: 0.0738\tvalidation loss: 0.0610\n",
      "epoch: 48\ttraining loss: 0.0740\tvalidation loss: 0.0623\n",
      "epoch: 49\ttraining loss: 0.0735\tvalidation loss: 0.0614\n",
      "epoch: 50\ttraining loss: 0.0732\tvalidation loss: 0.0616\n",
      "epoch: 51\ttraining loss: 0.0732\tvalidation loss: 0.0604\n",
      "epoch: 52\ttraining loss: 0.0730\tvalidation loss: 0.0612\n",
      "epoch: 53\ttraining loss: 0.0734\tvalidation loss: 0.0598\n",
      "epoch: 54\ttraining loss: 0.0732\tvalidation loss: 0.0618\n",
      "epoch: 55\ttraining loss: 0.1052\tvalidation loss: 0.0921\n",
      "epoch: 56\ttraining loss: 0.2914\tvalidation loss: 0.0939\n",
      "epoch: 57\ttraining loss: 0.2866\tvalidation loss: 0.0980\n",
      "epoch: 58\ttraining loss: 0.2315\tvalidation loss: 0.0925\n",
      "epoch: 59\ttraining loss: 0.1335\tvalidation loss: 0.1012\n",
      "epoch: 60\ttraining loss: 0.1242\tvalidation loss: 0.0920\n",
      "epoch: 61\ttraining loss: 0.1244\tvalidation loss: 0.0925\n",
      "epoch: 62\ttraining loss: 0.1239\tvalidation loss: 0.0925\n",
      "epoch: 63\ttraining loss: 0.1238\tvalidation loss: 0.0915\n",
      "epoch: 64\ttraining loss: 0.1240\tvalidation loss: 0.0935\n",
      "epoch: 65\ttraining loss: 0.1243\tvalidation loss: 0.0896\n",
      "epoch: 66\ttraining loss: 0.1239\tvalidation loss: 0.0904\n",
      "epoch: 67\ttraining loss: 0.1240\tvalidation loss: 0.0911\n",
      "epoch: 68\ttraining loss: 0.1242\tvalidation loss: 0.0903\n",
      "epoch: 69\ttraining loss: 0.1239\tvalidation loss: 0.0941\n",
      "epoch: 70\ttraining loss: 0.1238\tvalidation loss: 0.0939\n",
      "epoch: 71\ttraining loss: 0.1236\tvalidation loss: 0.0924\n",
      "epoch: 72\ttraining loss: 0.1235\tvalidation loss: 0.0930\n",
      "epoch: 73\ttraining loss: 0.1239\tvalidation loss: 0.0934\n",
      "epoch: 74\ttraining loss: 0.1236\tvalidation loss: 0.0932\n",
      "epoch: 75\ttraining loss: 0.1239\tvalidation loss: 0.0913\n",
      "epoch: 76\ttraining loss: 0.1240\tvalidation loss: 0.0938\n",
      "epoch: 77\ttraining loss: 0.1243\tvalidation loss: 0.0918\n",
      "epoch: 78\ttraining loss: 0.1237\tvalidation loss: 0.0935\n",
      "epoch: 79\ttraining loss: 0.1235\tvalidation loss: 0.0948\n",
      "epoch: 80\ttraining loss: 0.1240\tvalidation loss: 0.0920\n",
      "epoch: 81\ttraining loss: 0.1244\tvalidation loss: 0.0928\n",
      "epoch: 82\ttraining loss: 0.1241\tvalidation loss: 0.0921\n",
      "epoch: 83\ttraining loss: 0.1607\tvalidation loss: 0.0837\n",
      "epoch: 84\ttraining loss: 0.1287\tvalidation loss: 0.0908\n",
      "epoch: 85\ttraining loss: 0.1243\tvalidation loss: 0.0958\n",
      "epoch: 86\ttraining loss: 0.1238\tvalidation loss: 0.0939\n",
      "epoch: 87\ttraining loss: 0.1241\tvalidation loss: 0.0915\n",
      "epoch: 88\ttraining loss: 0.1197\tvalidation loss: 0.0783\n",
      "epoch: 89\ttraining loss: 0.3310\tvalidation loss: 0.0625\n",
      "epoch: 90\ttraining loss: 0.2890\tvalidation loss: 0.0911\n",
      "epoch: 91\ttraining loss: 0.1243\tvalidation loss: 0.0849\n",
      "epoch: 92\ttraining loss: 0.1246\tvalidation loss: 0.0917\n",
      "epoch: 93\ttraining loss: 0.1237\tvalidation loss: 0.0898\n",
      "epoch: 94\ttraining loss: 0.1238\tvalidation loss: 0.0929\n",
      "epoch: 95\ttraining loss: 0.1240\tvalidation loss: 0.0935\n",
      "epoch: 96\ttraining loss: 0.1241\tvalidation loss: 0.0902\n",
      "epoch: 97\ttraining loss: 0.1237\tvalidation loss: 0.0914\n",
      "epoch: 98\ttraining loss: 0.1243\tvalidation loss: 0.0921\n",
      "epoch: 99\ttraining loss: 0.1239\tvalidation loss: 0.0942\n",
      "epoch: 100\ttraining loss: 0.1242\tvalidation loss: 0.0916\n",
      "Model training took 16.301 seconds\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "myenv",
   "language": "python",
   "display_name": "Python (myenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
