{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import sys\n",
    "import csv\n",
    "from BDDData import *\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import importlib\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from joblib import Parallel, delayed\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bdd_data = BDD_dataset(\"raw_data/\")\n",
    "# bdd_data.add_timestep_id()\n",
    "# bdd_data.tag_chaotic(replace=True)\n",
    "\n",
    "# bdd_data.interpolate_power()\n",
    "# bdd_data.cap_power_to_zero()\n",
    "# bdd_data.normalize_power(min=0, max=1, method= \"MinMaxScaler\")\n",
    "# train, val, test = bdd_data.split_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd_data = BDD_dataset(\"raw_data/\")\n",
    "bdd_data.add_timestep_id()\n",
    "bdd_data.tag_chaotic(replace=True)\n",
    "bdd_data.interpolate_power()\n",
    "bdd_data.cap_power_to_zero()\n",
    "bdd_data.normalize_power(min=0, max=1, method= \"MinMaxScaler\")\n",
    "_, _, test = bdd_data.split_df()\n",
    "\n",
    "test_dataset_nan = CustomBDD_Dataset(\"test\",observation_window=obs_window,forecast_window=forecast_window)\n",
    "test_loader_nan = data.DataLoader(test_dataset_nan, shuffle=False, batch_size = batch_size)\n",
    "\n",
    "bdd_data = BDD_dataset(\"raw_data/\")\n",
    "bdd_data.add_timestep_id()\n",
    "bdd_data.tag_chaotic(replace=True)\n",
    "bdd_data.interpolate_power()\n",
    "bdd_data.cap_power_to_zero()\n",
    "bdd_data.normalize_power(min=0, max=1, method= \"MinMaxScaler\")\n",
    "_, _, test = bdd_data.split_df()\n",
    "\n",
    "test_dataset_interpolated = CustomBDD_Dataset(\"test\",observation_window=obs_window,forecast_window=forecast_window)\n",
    "test_loader_interpolated = data.DataLoader(test_dataset_interpolated, shuffle=False, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBDD_Dataset(data.Dataset):\n",
    "    def __init__(self, dataset, observation_window=12, forecast_window=12, starting_turbine = 0,  ending_turbine=133):\n",
    "        self.observation_window = observation_window\n",
    "        self.forecast_window = forecast_window\n",
    "        length = eval(f'len({dataset}[0])')\n",
    "        bdd_data.get_observation_forecasting_window(time_series_len=length, observation_steps=self.observation_window, forecast_steps=self.forecast_window)\n",
    "        self.window_of_interest =  bdd_data.sliding_indices[str(self.observation_window)+\",\"+str(self.forecast_window)]\n",
    "        self.starting_turbine = starting_turbine\n",
    "        self.ending_turbine = ending_turbine  \n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.window_of_interest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window = self.window_of_interest[idx]\n",
    "        if self.dataset == \"train\":\n",
    "            features = train[self.starting_turbine:self.ending_turbine+1,window[0]:window[1]]#.transpose().reshape(-1, 1)\n",
    "            labels = train[self.starting_turbine:self.ending_turbine+1,window[1]:window[2]]#.transpose().reshape(-1, 1)\n",
    "        elif self.dataset == \"val\":\n",
    "            features = val[self.starting_turbine:self.ending_turbine+1,window[0]:window[1]]#.transpose().reshape(-1, 1)\n",
    "            labels = val[self.starting_turbine:self.ending_turbine+1,window[1]:window[2]]#.transpose().reshape(-1, 1)\n",
    "        elif self.dataset == \"test\":\n",
    "            features = test[self.starting_turbine:self.ending_turbine+1,window[0]:window[1]]#.transpose().reshape(-1, 1)\n",
    "            labels = test[self.starting_turbine:self.ending_turbine+1,window[1]:window[2]]#.transpose().reshape(-1, 1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return torch.from_numpy(features).float(), torch.from_numpy(labels).float()\n",
    "    \n",
    "obs_window = 12\n",
    "forecast_window = 12\n",
    "batch_size = 100\n",
    "\n",
    "# train_dataset = CustomBDD_Dataset(\"train\",observation_window=obs_window,forecast_window=forecast_window)\n",
    "# train_loader = data.DataLoader(train_dataset, shuffle=True, batch_size = batch_size)\n",
    "# val_dataset = CustomBDD_Dataset(\"val\",observation_window=obs_window,forecast_window=forecast_window)\n",
    "# val_loader = data.DataLoader(val_dataset, shuffle=True, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "class SDELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SDELoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        error = input - target\n",
    "        mean_error = torch.mean(error)\n",
    "        return torch.sqrt(torch.mean((error - mean_error) ** 2))\n",
    "    \n",
    "class MaskedLoss(nn.Module):\n",
    "    def __init__(self, criterion):\n",
    "        super(MaskedLoss, self).__init__()\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        # Create a mask that is 1 for non-NaN entries and 0 for NaN entries\n",
    "        mask = ~torch.isnan(target)\n",
    "        # Apply the mask to only keep non-NaN elements\n",
    "        out = prediction[mask]\n",
    "        tar = target[mask]\n",
    "        # Calculate MSE Loss on non-NaN elements\n",
    "        return self.criterion(out, tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to run ARIMA\n",
    "def fit_arima_and_forecast(series, order):\n",
    "    model = ARIMA(series, order=order)\n",
    "    model_fit = model.fit()\n",
    "    forecast = model_fit.forecast(steps=12)\n",
    "    return forecast\n",
    "\n",
    "def arima(order, interpolated_loader, nan_loader):\n",
    "    limit = len(interpolated_loader)\n",
    "    total_rmse_loss = 0\n",
    "    total_mae_loss = 0\n",
    "    total_sde_loss = 0\n",
    "    counter = 0\n",
    "    n_jobs = -1\n",
    "    \n",
    "    mse_loss = MaskedLoss(nn.MSELoss())\n",
    "    mae_loss = MaskedLoss(nn.L1Loss())\n",
    "    sde_loss = MaskedLoss(SDELoss())\n",
    "    \n",
    "    nan_iter = iter(nan_loader)\n",
    "    \n",
    "    for x, _ in interpolated_loader:\n",
    "        _, y = next(nan_iter)\n",
    "        counter += 1\n",
    "        print(counter)\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        flat_x = x.view(-1, 12).tolist()\n",
    "        results = Parallel(n_jobs=n_jobs)(delayed(fit_arima_and_forecast)(series, order) for series in flat_x)\n",
    "    \n",
    "        results = np.array(results).reshape(batch_size, 134, 12)\n",
    "        results = torch.from_numpy(results).float()\n",
    "        \n",
    "        mse = mse_loss.forward(results, y)\n",
    "        mae = mae_loss.forward(results, y)\n",
    "        sde = sde_loss.forward(results, y)\n",
    "        \n",
    "        rmse = torch.sqrt(mse)\n",
    "        \n",
    "        total_rmse_loss += rmse.item()\n",
    "        total_mae_loss += mae.item()\n",
    "        total_sde_loss += sde.item()\n",
    "        \n",
    "        # if counter == 1:\n",
    "        #     break\n",
    "    print(f'RMSE: {total_rmse_loss / limit}, MAE: {total_mae_loss / limit}, SDE: {total_sde_loss / limit}')\n",
    "    return total_rmse_loss / limit, total_mae_loss / limit, total_sde_loss / limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "# Run ARIMA on the train set with order (1, 0, 0) (AR(1), I(0), MA(0))\n",
    "arima((1, 0, 0), test_loader_interpolated, test_loader_nan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
