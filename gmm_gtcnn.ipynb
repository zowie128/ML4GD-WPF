{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "sys.path.append('..')\n",
    "from BDDData import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import circulant\n",
    "from product_graph import *\n",
    "from utils import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134, 24624)\n"
     ]
    }
   ],
   "source": [
    "#Load dataframes\n",
    "bdd_data = BDD_dataset(\"raw_data/BDDdata/\")\n",
    "#Add column with the timestep\n",
    "bdd_data.add_timestep_id()\n",
    "#Add flags for chaotic values\n",
    "bdd_data.tag_chaotic(replace=True)\n",
    "#Compute the mod for the nazelle and wind angles\n",
    "bdd_data.angle_mod()\n",
    "\n",
    "# ! Don't interpolate this data\n",
    "# ! Missing values remain NaN\n",
    "# #Interpolate the missing values\n",
    "# bdd_data.interpolate_power()\n",
    "\n",
    "#Values smaller than 0 are set to 0\n",
    "bdd_data.cap_power_to_zero()\n",
    "#Normalize Patv feature to [0,1]\n",
    "bdd_data.normalize_power(min=0, max=1, method= \"MinMaxScaler\")\n",
    "#Convert df to matrix form, where only Patv is included. Then split into train, validation and test\n",
    "#The matrix contains the subset of the time series for ALL nodes, so an (TxN matrix)\n",
    "train, val, test = bdd_data.split_df()\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "np.random.seed(42)\n",
    "# consider a p percentage of the data\n",
    "p = 0.1\n",
    "train_mask = np.random.choice(train.shape[1], int(train.shape[1] * p), replace=False)\n",
    "val_mask = np.random.choice(val.shape[1], int(val.shape[1] * p), replace=False)\n",
    "test_mask = np.random.choice(test.shape[1], int(test.shape[1] * p), replace=False)\n",
    "\n",
    "train = train[:, train_mask]\n",
    "val = val[:, val_mask]\n",
    "test = test[:, test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBDD_Dataset(data.Dataset):\n",
    "    def __init__(self, dataset, observation_window=12, forecast_window=12, starting_turbine = 0,  ending_turbine=133):\n",
    "        self.observation_window = observation_window\n",
    "        self.forecast_window = forecast_window\n",
    "        length = eval(f'len({dataset}[0])')#Retrieves length of dataset\n",
    "        bdd_data.get_observation_forecasting_window(time_series_len=length, observation_steps=self.observation_window, forecast_steps=self.forecast_window)#Generates obs window\n",
    "        self.window_of_interest =  bdd_data.sliding_indices[str(self.observation_window)+\",\"+str(self.forecast_window)]#Retrieves windows\n",
    "        self.starting_turbine = starting_turbine\n",
    "        self.ending_turbine = ending_turbine  \n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.window_of_interest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window = self.window_of_interest[idx]\n",
    "        if self.dataset == \"train\":\n",
    "            features = train[self.starting_turbine:self.ending_turbine+1,window[0]:window[1]].transpose().reshape(-1, 1)\n",
    "            labels = train[self.starting_turbine:self.ending_turbine+1,window[1]:window[2]].transpose().reshape(-1, 1)\n",
    "        elif self.dataset == \"val\":\n",
    "            features = val[self.starting_turbine:self.ending_turbine+1,window[0]:window[1]].transpose().reshape(-1, 1)\n",
    "            labels = val[self.starting_turbine:self.ending_turbine+1,window[1]:window[2]].transpose().reshape(-1, 1)\n",
    "        elif self.dataset == \"test\":\n",
    "            features = test[self.starting_turbine:self.ending_turbine+1,window[0]:window[1]].transpose().reshape(-1, 1)\n",
    "            labels = test[self.starting_turbine:self.ending_turbine+1,window[1]:window[2]].transpose().reshape(-1, 1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return torch.from_numpy(features).float(), torch.from_numpy(labels).float()\n",
    "    \n",
    "obs_window = 12\n",
    "forecast_window = 12\n",
    "batch_size = 100\n",
    "\n",
    "train_dataset = CustomBDD_Dataset(\"train\",observation_window=obs_window,forecast_window=forecast_window)\n",
    "train_loader = data.DataLoader(train_dataset, shuffle=True, batch_size = batch_size)\n",
    "val_dataset = CustomBDD_Dataset(\"val\",observation_window=obs_window,forecast_window=forecast_window)\n",
    "val_loader = data.DataLoader(val_dataset, shuffle=True, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=torch.Size([100, 1608, 1])\n",
      "y.shape=torch.Size([100, 1608, 1])\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(train_loader))\n",
    "print(f\"{x.shape=}\\n{y.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gml('data/spatial_graph_2000.gml')\n",
    "adj_mat = nx.adjacency_matrix(G)\n",
    "adj_mat = nx.to_numpy_array(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_time_graph(window: int, directed: bool, cyclic: bool):\n",
    "    \"\"\"\n",
    "    Circulant matrix as in https://arxiv.org/pdf/1712.00468.pdf (eq. 7)\n",
    "    \"\"\"\n",
    "    if window <= 1:\n",
    "        raise Exception(\"Ehm..\")\n",
    "    adjacency = circulant([0, 1] + [0 for _ in range(window-2)])\n",
    "    if not cyclic:\n",
    "        adjacency[0, window-1] = 0\n",
    "    if not directed:\n",
    "        adjacency += adjacency.transpose()\n",
    "\n",
    "    return adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timegraph = build_time_graph(12,True,False)\n",
    "print(timegraph)\n",
    "tg = nx.from_numpy_array(timegraph)\n",
    "nx.draw(tg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the adjacency matrix\n",
    "S = normalize_adjacency(torch.tensor(adj_mat)).float()\n",
    "S_t = torch.tensor(timegraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = build_parametric_product_graph(S_t, S, 0, 1, 1, 1).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T11:05:24.457246Z",
     "start_time": "2024-06-27T11:05:24.135510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1608, 1608])\n"
     ]
    }
   ],
   "source": [
    "print(pg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR GMM Implementation\n",
    "# ! Taken from the paper.\n",
    "def ex_relu(mu, sigma):\n",
    "    is_zero = sigma == 0\n",
    "    sigma[is_zero] = 1e-10\n",
    "    sqrt_sigma = torch.sqrt(sigma)\n",
    "    w = torch.div(mu, sqrt_sigma)\n",
    "    nr_values = sqrt_sigma * (\n",
    "        torch.div(torch.exp(torch.div(-w * w, 2)), np.sqrt(2 * np.pi))\n",
    "        + torch.div(w, 2) * (1 + torch.erf(torch.div(w, np.sqrt(2))))\n",
    "    )\n",
    "    nr_values = torch.where(is_zero, F.relu(mu), nr_values)\n",
    "    return nr_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMMGCNLayer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        num_components,\n",
    "        all_features,\n",
    "        all_A,\n",
    "        device=\"cpu\",\n",
    "    ) -> None:\n",
    "        super(GMMGCNLayer, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.num_components = num_components\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # All feature data, used for initialization of the GMM.\n",
    "        self.all_features = all_features\n",
    "        self.A2 = torch.mul(all_A, all_A).to(self.device)\n",
    "\n",
    "        # Initialize the weights\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        # Initialize GMM and its parameters (we're going to learn these)\n",
    "        self.gmm = self._init_gmm(self.all_features, self.num_components)\n",
    "        self.pi = nn.Parameter(torch.FloatTensor(np.log(self.gmm.weights_))).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.mu = nn.Parameter(torch.FloatTensor(self.gmm.means_).to(self.device))\n",
    "        self.sigma = nn.Parameter(\n",
    "            torch.FloatTensor(np.log(self.gmm.covariances_)).to(self.device)\n",
    "        )\n",
    "\n",
    "    def _init_gmm(self, features, K):\n",
    "        # Simply impute the values once for initialization of the gmm.\n",
    "        # Keep empty features\n",
    "        imputer = SimpleImputer(\n",
    "            missing_values=np.nan, strategy=\"mean\", keep_empty_features=True\n",
    "        )\n",
    "        imputed_x = imputer.fit_transform(features)\n",
    "        gmm = GaussianMixture(n_components=K, covariance_type=\"diag\").fit(imputed_x)\n",
    "        return gmm\n",
    "\n",
    "    # ! Taken from the paper.\n",
    "    def _calc_responsibility(self, mean_mat, variances):\n",
    "        dim = self.in_features\n",
    "        log_n = (\n",
    "            (-1 / 2)\n",
    "            * torch.sum(\n",
    "                torch.pow(mean_mat - self.mu.unsqueeze(1), 2) / variances.unsqueeze(1),\n",
    "                2,\n",
    "            )\n",
    "            - (dim / 2) * np.log(2 * np.pi)\n",
    "            - (1 / 2) * torch.sum(self.sigma)\n",
    "        )\n",
    "        log_prob = self.pi.unsqueeze(1) + log_n\n",
    "        return torch.softmax(log_prob, dim=0)\n",
    "\n",
    "    def forward(self, shift, features):\n",
    "        batch_size = features.size(0)\n",
    "        tensor_list = []\n",
    "        for i in range(batch_size):\n",
    "            out = self._forward(shift, features[i])\n",
    "            tensor_list.append(out)\n",
    "        return torch.stack(tensor_list, dim=0)\n",
    "\n",
    "    # ! Taken from the paper.\n",
    "    def _forward(self, shift, features):\n",
    "        x_imp = features.repeat(self.num_components, 1, 1)\n",
    "        x_isnan = torch.isnan(x_imp)\n",
    "        # x_isnan = torch.is_zero(x_imp)\n",
    "        variances = torch.exp(self.sigma)\n",
    "        # M\n",
    "        mean_mat = torch.where(\n",
    "            x_isnan, self.mu.repeat((features.size(0), 1, 1)).permute(1, 0, 2), x_imp\n",
    "        )\n",
    "        # S\n",
    "        var_mat = torch.where(\n",
    "            x_isnan,\n",
    "            variances.repeat((features.size(0), 1, 1)).permute(1, 0, 2),\n",
    "            torch.zeros(size=x_imp.size(), device=self.device, requires_grad=True),\n",
    "        )\n",
    "\n",
    "        # M^kW\n",
    "        transform_x = torch.matmul(mean_mat, self.weight)\n",
    "        # S^k (W * W)\n",
    "        transform_covs = torch.matmul(var_mat, self.weight * self.weight)\n",
    "        conv_x = []\n",
    "        conv_covs = []\n",
    "        for component_x in transform_x:\n",
    "            # LM^kW\n",
    "            conv_x.append(torch.spmm(shift, component_x))\n",
    "        for component_covs in transform_covs:\n",
    "            # (L*L) S^k (W * W)\n",
    "            conv_covs.append(torch.spmm(self.A2, component_covs))\n",
    "\n",
    "        transform_x = torch.stack(conv_x, dim=0)\n",
    "        transform_covs = torch.stack(conv_covs, dim=0)\n",
    "        # ReLU[N(M, S)]\n",
    "        expected_x = ex_relu(transform_x, transform_covs)\n",
    "\n",
    "        # calculate responsibility\n",
    "        gamma = self._calc_responsibility(mean_mat, variances)\n",
    "        # ReLU[(LXW)]\n",
    "        expected_x = torch.sum(expected_x * gamma.unsqueeze(2), dim=0)\n",
    "        return expected_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMMGCNNLayer(GMMGCNLayer):\n",
    "    \"\"\"This extends the GMMGCNLayer, adding order q. Forward pass LMW becomes Sum over q of L^qMW.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        num_components,\n",
    "        all_features,\n",
    "        all_A,\n",
    "        order,\n",
    "        device=\"cpu\",\n",
    "    ) -> None:\n",
    "        super(GMMGCNNLayer, self).__init__(\n",
    "            in_features, out_features, num_components, all_features, all_A, device\n",
    "        )\n",
    "        self.all_A = all_A\n",
    "        self.order = order\n",
    "\n",
    "    def forward(self, shift, features):\n",
    "        batch_size = features.size(0)\n",
    "        # output_dim = self.weights.size(1)\n",
    "        # shift_powers = [torch.matrix_power(shift, k).float() for k in range(self.order)]\n",
    "\n",
    "        tensor_list = []\n",
    "        for i in range(batch_size):\n",
    "            out = self._forward(shift, features[i])\n",
    "            tensor_list.append(out)\n",
    "        return torch.stack(tensor_list, dim=0)\n",
    "\n",
    "    def _forward(self, shift, features):\n",
    "        x_imp = features.repeat(self.num_components, 1, 1)\n",
    "        x_isnan = torch.isnan(x_imp)\n",
    "        variances = torch.exp(self.sigma)\n",
    "        # M\n",
    "        mean_mat = torch.where(\n",
    "            x_isnan, self.mu.repeat((features.size(0), 1, 1)).permute(1, 0, 2), x_imp\n",
    "        )\n",
    "        # S\n",
    "        var_mat = torch.where(\n",
    "            x_isnan,\n",
    "            variances.repeat((features.size(0), 1, 1)).permute(1, 0, 2),\n",
    "            torch.zeros(size=x_imp.size(), device=self.device, requires_grad=True),\n",
    "        )\n",
    "\n",
    "        # M^kW\n",
    "        transform_x = torch.matmul(mean_mat, self.weight)\n",
    "        # S^k (W * W)\n",
    "        transform_covs = torch.matmul(var_mat, self.weight * self.weight)\n",
    "        conv_x = []\n",
    "        conv_covs = []\n",
    "        for component_x in transform_x:\n",
    "            # First:\n",
    "            # LM^kW\n",
    "            # conv_x.append(torch.spmm(shift, component_x))\n",
    "\n",
    "            # Becomes:\n",
    "            # sum over Q of (L^q M^k W)\n",
    "            out = torch.zeros(component_x.size(0), self.weight.size(1))\n",
    "            # compute order hop shift\n",
    "            for q in range(self.order):\n",
    "                out += (\n",
    "                    torch.spmm(torch.matrix_power(shift, q), component_x)\n",
    "                ) / self.order\n",
    "            conv_x.append(out)\n",
    "\n",
    "        for component_covs in transform_covs:\n",
    "            # First:\n",
    "            # (L*L) S^k (W * W)\n",
    "            # conv_covs.append(torch.spmm(self.A2, component_covs))\n",
    "\n",
    "            # Becomes:\n",
    "            # sum over Q of ( (L^q * L^q) S^k (W * W) )\n",
    "            out = torch.zeros(component_covs.size(0), self.weight.size(1))\n",
    "            # compute order hop shift\n",
    "            for q in range(self.order):\n",
    "                # A2 explodes to zero\n",
    "                A2 = torch.mul(\n",
    "                    torch.matrix_power(self.all_A, q), torch.matrix_power(self.all_A, q)\n",
    "                ).to(self.device)\n",
    "\n",
    "                out += (torch.spmm(A2, component_covs)) / self.order\n",
    "\n",
    "            conv_covs.append(out)\n",
    "\n",
    "        transform_x = torch.stack(conv_x, dim=0)\n",
    "        transform_covs = torch.stack(conv_covs, dim=0)\n",
    "        # ReLU[N(M, S)]\n",
    "        expected_x = ex_relu(transform_x, transform_covs)\n",
    "\n",
    "        # calculate responsibility\n",
    "        gamma = self._calc_responsibility(mean_mat, variances)\n",
    "        # ReLU[(LXW)]\n",
    "        expected_x = torch.sum(expected_x * gamma.unsqueeze(2), dim=0)\n",
    "        # Check for NaNs\n",
    "        if torch.isnan(expected_x).any():\n",
    "            print(\"NaN detected in expected_x:\")\n",
    "        return expected_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, matrix_powers, order):\n",
    "        super(GCNNLayer, self).__init__()\n",
    "        self.matrix_powers = matrix_powers\n",
    "        self.order = order\n",
    "        self.weights = nn.Parameter(torch.FloatTensor(in_features, out_features, order))\n",
    "        # use Xavier initialization to match variance of input with output\n",
    "        nn.init.xavier_uniform_(self.weights)\n",
    "\n",
    "    def forward(self, features):\n",
    "        batch_size = features.size(0)\n",
    "        output_dim = self.weights.size(1)\n",
    "        device = features.device\n",
    "\n",
    "        out = torch.zeros((batch_size, features.size(1), output_dim), device=device)\n",
    "        for k in range(self.order):\n",
    "            weighted = torch.bmm(features, self.weights[:, :, k].unsqueeze(0).repeat(batch_size, 1, 1))\n",
    "            shifted = torch.bmm(self.matrix_powers[k].unsqueeze(0).repeat(batch_size, 1, 1).to(device), weighted)\n",
    "            out += shifted\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMMGCNN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, obs_size, pred_size, hid_sizes, num_components, all_features, all_A, order\n",
    "    ):\n",
    "        super(GMMGCNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # First layer is a GCNN that incorporates the GMM.\n",
    "        self.layers.append(\n",
    "            GMMGCNNLayer(\n",
    "                obs_size, hid_sizes[0], num_components, all_features, all_A, order\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Later layers are regular GCNs.\n",
    "        # num_hid hidden layers of size hid_size\n",
    "        for i in range(len(hid_sizes) - 1):\n",
    "            self.layers.append(GCNNLayer(hid_sizes[i], hid_sizes[i + 1], [torch.eye(all_A.size(0)), all_A], order))\n",
    "        # fully connected layer to get  output of dim pred_size\n",
    "        self.layers.append(GCNNLayer(hid_sizes[-1], pred_size, [torch.eye(all_A.size(0)), all_A], order))\n",
    "\n",
    "    def forward(self, shift, features):\n",
    "        # print(f\"forward in: {features}\")\n",
    "        temp = features\n",
    "        # No relu for the first layer, it is a GMM layer.\n",
    "        temp = self.layers[0].forward(shift, features)\n",
    "        # print(f\"after gmmgcnn layer: {temp}\")\n",
    "        for layer in self.layers[1:-1]:\n",
    "            # use relu activation function\n",
    "            # No shift operator necessary for the GCNN, it is already added to the initatialization.\n",
    "            temp = F.relu(layer(temp))\n",
    "        # Last layer no relu\n",
    "        return self.layers[-1](temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        # Create a mask that is 1 for non-NaN entries and 0 for NaN entries\n",
    "        mask = ~torch.isnan(target)\n",
    "        # Apply the mask to only keep non-NaN elements\n",
    "        out = prediction[mask]\n",
    "        tar = target[mask]\n",
    "        # Calculate MSE Loss on non-NaN elements\n",
    "        return nn.functional.mse_loss(out, tar)\n",
    "\n",
    "def train_epoch_gcnn(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        # Remove dimension of size 1.\n",
    "        # ! Does not work yet with batches\n",
    "        # x = x[0, :, :]\n",
    "        # y = y[0, :, :]\n",
    "\n",
    "        out = model(pg, x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_epoch_gcnn(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        # Remove dimension of size 1.\n",
    "        # ! Does not work yet with batches\n",
    "        # x = x[0, :, :]\n",
    "        # y = y[0, :, :]\n",
    "        out = model(pg, x)\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def train_gcnn(model, num_epochs, criterion, train_loader, test_loader):\n",
    "    # TODO: Check loss function!\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(epoch)\n",
    "        # Model training\n",
    "        train_loss = train_epoch_gcnn(model, train_loader, optimizer, criterion)\n",
    "\n",
    "        # Model validation\n",
    "        val_loss = evaluate_epoch_gcnn(model, test_loader, criterion)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(\n",
    "                \"epoch:\",\n",
    "                epoch,\n",
    "                \"\\t training loss:\",\n",
    "                np.round(train_loss, 4),\n",
    "                \"\\t validation loss:\",\n",
    "                np.round(val_loss, 4),\n",
    "            )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Model training took {elapsed_time:.3f} seconds\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16655\n",
      "layers.0.weight torch.Size([1, 128])\n",
      "layers.0.pi torch.Size([5])\n",
      "layers.0.mu torch.Size([5, 1])\n",
      "layers.0.sigma torch.Size([5, 1])\n",
      "layers.1.weights torch.Size([128, 128, 1])\n",
      "layers.2.weights torch.Size([128, 1, 1])\n",
      "1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(name, param\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 14\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMaskedMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[85], line 61\u001b[0m, in \u001b[0;36mtrain_gcnn\u001b[0;34m(model, num_epochs, criterion, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(epoch)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Model training\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch_gcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Model validation\u001b[39;00m\n\u001b[1;32m     64\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m evaluate_epoch_gcnn(model, test_loader, criterion)\n",
      "Cell \u001b[0;32mIn[85], line 25\u001b[0m, in \u001b[0;36mtrain_epoch_gcnn\u001b[0;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     18\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Remove dimension of size 1.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# ! Does not work yet with batches\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# x = x[0, :, :]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# y = y[0, :, :]\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out, y)\n\u001b[1;32m     27\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/cse-ml4gd/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cse-ml4gd/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[84], line 32\u001b[0m, in \u001b[0;36mGMMGCNN.forward\u001b[0;34m(self, shift, features)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# print(f\"after gmmgcnn layer: {temp}\")\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# use relu activation function\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# No shift operator necessary for the GCNN, it is already added to the initatialization.\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     temp \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Last layer no relu\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](temp)\n",
      "File \u001b[0;32m~/miniconda3/envs/cse-ml4gd/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cse-ml4gd/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 18\u001b[0m, in \u001b[0;36mGCNNLayer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morder):\n\u001b[1;32m     17\u001b[0m     weighted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[:, :, k]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 18\u001b[0m     shifted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix_powers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), weighted)\n\u001b[1;32m     19\u001b[0m     out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m shifted\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "all_features = train.reshape(-1, 1)\n",
    "all_A = pg\n",
    "\n",
    "n_components = 5\n",
    "order = 1\n",
    "num_epochs = 1\n",
    "model = GMMGCNN(obs_size=1, pred_size=1, hid_sizes=[128, 128], num_components=n_components, all_features=all_features, all_A=all_A, order=order)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.size())\n",
    "train_losses, val_losses = train_gcnn(model, num_epochs, MaskedMSELoss(), train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse-ml4gd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
